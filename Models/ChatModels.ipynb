{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3490f060",
   "metadata": {},
   "source": [
    "# Chat Model\n",
    "### This is a specialized LLM that is an expert at having conversations. It's designed to be good at back-and-forth dialogue, remembering what you've talked about to have a natural chat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49bc69c",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33e98ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='A large language model is a type of artificial intelligence model that has been trained on a vast amount of text data. It is designed to generate human-like text based on the input it receives. These models are \"large\" because they have a high number of parameters, often in the billions, which allows them to better understand and generate complex language patterns. Examples of large language models include OpenAI\\'s GPT-3 and Google\\'s BERT.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 13, 'total_tokens': 102, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BvnpGDFmWXZPjCRo0I8v8YKJOrnso', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--3a42f7dd-7c84-43a7-8f47-60d941a4151d-0' usage_metadata={'input_tokens': 13, 'output_tokens': 89, 'total_tokens': 102, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4', temperature=0.1, max_tokens=1000)\n",
    "\n",
    "result = model.invoke(\"What is Large language model?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59525bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model is a type of artificial intelligence model that has been trained on a vast amount of text data. It is designed to generate human-like text based on the input it receives. These models are \"large\" because they have a high number of parameters, often in the billions, which allows them to better understand and generate complex language patterns. Examples of large language models include OpenAI's GPT-3 and Google's BERT.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b287305",
   "metadata": {},
   "source": [
    "## Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "531bd19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run Anthropic models\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "# model = ChatAnthropic(modael='claude-2', temperature=0.1, max_tokens=1000)\n",
    "# result = model.invoke(\"What is Large language model?\")\n",
    "# print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d9a2d1",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fbdb328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Large Language Model (LLM) is a type of artificial intelligence (AI) that can understand and generate human-like text.  It's \"large\" because it's trained on a massive dataset of text and code, often containing trillions of words. This massive dataset allows the model to learn patterns, relationships, and nuances in language, enabling it to perform a wide range of tasks.\n",
      "\n",
      "Here's a breakdown of key aspects:\n",
      "\n",
      "* **Massive Dataset:**  The sheer size of the training data is crucial.  The more data, the better the model can understand and generate text.  This data includes books, articles, code, websites, and more.\n",
      "\n",
      "* **Neural Networks:** LLMs are built using deep learning techniques, specifically neural networks with many layers (hence \"deep learning\"). These networks are designed to identify patterns and relationships within the data.\n",
      "\n",
      "* **Transformer Architecture:** Most modern LLMs utilize the transformer architecture. This architecture is particularly effective at processing sequential data like text, allowing the model to understand context and relationships between words in a sentence or paragraph.\n",
      "\n",
      "* **Predictive Capabilities:**  At its core, an LLM predicts the next word in a sequence.  This seemingly simple task, when combined with the massive dataset and sophisticated architecture, allows for complex tasks like:\n",
      "    * **Text generation:** Writing stories, articles, summaries, code, etc.\n",
      "    * **Translation:** Converting text from one language to another.\n",
      "    * **Question answering:** Providing answers to questions based on its knowledge.\n",
      "    * **Chatbots:** Engaging in conversations with users.\n",
      "    * **Summarization:** Condensing large amounts of text into shorter summaries.\n",
      "\n",
      "* **Limitations:**  Despite their capabilities, LLMs have limitations. They can sometimes generate inaccurate, nonsensical, or biased information.  They lack true understanding and reasoning abilities; they are essentially sophisticated pattern-matching machines.  They are also computationally expensive to train and run.\n",
      "\n",
      "\n",
      "In short, an LLM is a powerful tool for processing and generating human language, but it's important to be aware of its limitations and use it responsibly.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",  \n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    "    google_api_key=api_key \n",
    ")\n",
    "\n",
    "result = model.invoke(\"What is a Large Language Model?\")\n",
    "\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ba90b1",
   "metadata": {},
   "source": [
    "## Coher API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ba14cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model is a type of artificial neural network that has been trained on massive amounts of text data, typically in the billions or even trillions of words. These models use deep learning techniques to analyze and learn from the patterns, relationships, and context of human language. They can generate and understand human-like language, and are capable of a wide range of language tasks such as text generation, translation, summarization, question answering, and more.\n",
      "\n",
      "Some examples of large language models include:\n",
      "- GPT-3 (Generative Pre-trained Transformer 3) from OpenAI, which has 175 billion parameters and can generate human-like text based on a prompt.\n",
      "- BERT (Bidirectional Encoder Representations from Transformers) from Google, which is a bidirectional model that can understand the context of words based on the surrounding words, and is used for tasks like question answering and language understanding.\n",
      "- T5 (Text-to-Text Transfer Transformer) from Google, which frames all language tasks as a text-to-text problem and is trained on a large corpus of text.\n",
      "- RoBERTa (Robustly Optimized BERT Approach) from Facebook AI, which is a variant of BERT that uses more data and longer training times to improve performance.\n",
      "\n",
      "These models have revolutionized natural language processing and have a wide range of applications in areas such as language translation, content generation, customer service, language understanding, and more. They have also raised ethical concerns around bias, privacy, and misuse, which is an active area of research and discussion in the NLP community.\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "co = cohere.Client(api_key)\n",
    "\n",
    "response = co.chat(\n",
    "    message=\"What is a large language model?\",\n",
    "    temperature=0.3,\n",
    "    chat_history=[],\n",
    "    model=\"command-r-plus\"  # or command-r\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ead65",
   "metadata": {},
   "source": [
    "## Hugging Face Inference API \n",
    "## Open source model's access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d92a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f569d2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
